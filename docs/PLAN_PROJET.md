# Plan du Projet - QV-Pipe Classifier
### Objectif global :  
Am√©liorer la mAP de la classification multi-label des d√©fauts dans les vid√©os Quick-View (QV) via une approche optimis√©e en termes de donn√©es, de mod√®les, et de m√©thodes d'entra√Ænement.

---

##  **Vue d'ensemble**

L'objectif du projet est de cr√©er un mod√®le performant capable de classifier les d√©fauts pr√©sents dans les vid√©os QV-Pipe, un jeu de donn√©es d'inspection de canalisations.  
Pour ce faire, nous allons utiliser des techniques de deep learning adapt√©es aux vid√©os, tout en tenant compte des contraintes li√©es √† la taille des vid√©os et de leur nature d√©s√©quilibr√©e (long-tailed).

### **Contraintes principales :**  
- **Vid√©os lourdes :** Utilisation de techniques d'√©chantillonnage pour √©viter les mod√®les vid√©o trop co√ªteux.  
- **R√©partition multi-label et d√©s√©quilibr√©e des classes :** Prise en compte de ces caract√©ristiques avec des m√©thodes de perte adapt√©es (ASL, Class-Balanced Focal Loss).

---

##  **Objectifs du projet**

1. **Extraction des donn√©es :** Convertir les vid√©os en images utilisables pour entra√Æner un mod√®le efficace.
2. **Cr√©ation de super-images :** Construire des images composites 3√ó3 √† partir de frames √©chantillonn√©es pour augmenter la performance du mod√®le.
3. **Entra√Ænement du mod√®le :** Construire et entra√Æner plusieurs architectures deep learning adapt√©es √† la classification multi-label.
4. **Optimisation des hyperparam√®tres :** Utilisation de strat√©gies comme OneCycle, AdamW, et AMP pour un entra√Ænement rapide et stable.
5. **Am√©lioration des r√©sultats :** Utilisation de l'ensemble de plusieurs mod√®les pour maximiser la mAP.

---

##  **√âtapes du projet**

### **√âtape 1 : Pr√©paration des donn√©es**  
- **Objectif :** Transformer les vid√©os en images utiles et organiser les donn√©es pour l'entra√Ænement.
- **M√©thodes utilis√©es :**  
  - **Sampling uniforme :** Extraire 5 frames r√©guli√®rement espac√©es par vid√©o.  
  - **Nettoyage des donn√©es :**  
    - Supprimer les images floues (en utilisant la variance du Laplacien).
    - Supprimer les doublons (pHash).  
  - **Cr√©ation de splits stratifi√©s :**  
    - Split 5-fold multi-label en utilisant **iterative stratification** pour maintenir un √©quilibre des classes.
    - Normalisation des images avec les statistiques d'ImageNet (mean/std).

**D√©tails :**
- Nombre de vid√©os : 9 601 (55 heures de vid√©o).
- Chaque vid√©o contient des d√©fauts diff√©rents, avec des classes rares.

**Projets GitHub :**
- **[Decord](https://github.com/dmlc/decord)** - loader/lecture vid√©o ultra-rapide (FFmpeg/NV codecs). Parfait pour extraire des frames sans charger toute la vid√©o.  
- **[PyAV](https://github.com/PyAV-Org/PyAV)** - bindings FFmpeg en Python, flexible pour pipelines d‚Äôextraction personnalis√©s.  
- **[iterative-stratification](https://github.com/trent-b/iterative-stratification)** - cross-val **multi-label** (la stratification utilis√©e par les gagnants).  
- **[video2frame](https://github.com/jinyu121/video2frame)** - scripts simples d‚Äôextraction (uniforme, resize, multithread).  

---

### **√âtape 2 : Baseline frame-wise**  
- **Objectif :** Construire une baseline simple o√π 5 frames par vid√©o sont pass√©es dans un CNN (ResNet-18, TResNet).
- **M√©thode :**  
  - **CNN simple :** Utilisation de **ResNet-18** (pr√©-entra√Æn√© sur ImageNet).  
  - **Fusion vid√©o :** Moyenne des logits des 5 frames pour obtenir une pr√©diction vid√©o.
  - **Perte utilis√©e :**  
    - **BCE (Binary Cross-Entropy)** comme perte de base.
    - **ASL (Asymmetric Loss)** pour mieux g√©rer les classes d√©s√©quilibr√©es (pr√©sence de d√©fauts rares).
  - **Optimisation :**  
    - Optimiseur **AdamW** avec un planning de taux d'apprentissage **OneCycleLR**.
    - **AMP (half-precision)** pour acc√©l√©rer l'entra√Ænement.

**Projets GitHub :**
- **[timm](https://github.com/huggingface/pytorch-image-models)** ‚Äì zoo PyTorch (ResNet/TResNet, optim/schedulers, EMA) ; parfait pour baselines rapides.  
- **[TResNet (MIIL)](https://github.com/Alibaba-MIIL/TResNet)** ‚Äì architecture multi-label performante et l√©g√®re (r√©f. gagnants).  

---

### **√âtape 3 : Super-images 3√ó3**  
- **Objectif :** Cr√©er des super-images √† partir de 9 frames et entra√Æner un mod√®le plus complexe pour am√©liorer la mAP.
- **M√©thode :**  
  - **Super-image 3√ó3 :** √Ä partir des 9 frames extraites d‚Äôune vid√©o, cr√©er une grille de 3√ó3 (super-image).
  - **Mod√®les utilis√©s :**  
    - **ConvNeXt**, **NFNet**, **EfficientNet**, **TResNet-XL** + **MLDecoder** pour la t√™te multi-label.
  - **Pertes adapt√©es :**  
    - **ASL** ou **Class-Balanced Focal Loss** (CB-Focal) pour mieux g√©rer les classes rares.
  - **Augmentations :**  
    - **Horizontal Flip** et **Tile Shuffle** (permutation l√©g√®re des tuiles de la super-image).

**Projets GitHub :**
- **[IBM/sifar-pytorch](https://github.com/IBM/sifar-pytorch)** ‚Äì impl√©mentation ‚Äúsuper-image‚Äù (mise en grille 3√ó3/4√ó4 \+ entra√Ænement image-classifier).  
- **[timm](https://github.com/huggingface/pytorch-image-models)** ‚Äì backbones **ConvNeXt/NFNet/EfficientNet**, schedulers (**OneCycle**), **EMA** pr√™ts √† l‚Äôemploi.  
- **[ML-Decoder (head multi-label)](https://github.com/Alibaba-MIIL/ML_Decoder)** ‚Äì √† greffer sur un backbone timm si tu veux copier la t√™te gagnante TResNet-XL+MLDecoder.  
- **[ASL (Asymmetric Loss)](https://github.com/Alibaba-MIIL/ASL)** ‚Äì impl√©mentation officielle MIIL pour le multi-label long-tailed.  

---

### **√âtape 4 : Recette d‚Äôentra√Ænement stable**  
- **Objectif :** Optimiser l'entra√Ænement avec une strat√©gie stable et rapide.
- **M√©thode :**  
  - **AdamW** comme optimiseur.  
  - **OneCycleLR** pour ajuster le taux d'apprentissage.  
  - **AMP (half-precision)** pour acc√©l√©rer l‚Äôentra√Ænement en utilisant une pr√©cision r√©duite.
  - **EMA (Exponential Moving Average)** des poids pour am√©liorer la stabilit√©.
  - **Early stopping** sur la mAP pour arr√™ter l'entra√Ænement si la performance stagne.

**Projets GitHub :**
- **[timm (trainer \+ schedulers)](https://github.com/huggingface/pytorch-image-models)** ‚Äî **AdamW \+ OneCycle \+ AMP (fp16)**, **EMA**, callbacks et scripts reproductibles.  
- **[Class-Balanced Loss (effective number)](https://github.com/vandit15/Class-balanced-loss-pytorch)** ‚Äî impl√©mentations PyTorch pr√™tes pour **CB-Focal** (alternative/benchmark d‚ÄôASL).  

---

### **√âtape 5 : Ensemble de mod√®les**  
- **Objectif :** Am√©liorer la performance finale en combinant plusieurs mod√®les.
- **M√©thode :**  
  - **Ensemble des pr√©dictions :** Moyenne des pr√©dictions sur 5 folds (intra-mod√®le).
  - **Pond√©ration des mod√®les :** Utilisation de mod√®les diff√©rents (par exemple, NFNet, EffNet, ConvNeXt) et pond√©ration en fonction de leur mAP de validation.
  - **Post-traitement minimal :** Application d'une r√®gle de seuil (**ZC > 0.9 ‚Üí ZC = 1**) pour un l√©ger gain.

**Projets GitHub :**
- **[MMAction2](https://github.com/open-mmlab/mmaction2)** ‚Üí pour comparer plus tard avec un run Video Swin-B (Top2).

---

##  **Technologies et Frameworks**

- **Frameworks principaux :**
  - **PyTorch** pour le deep learning.
  - **timm (PyTorch Image Models)** pour les backbones pr√©-entra√Æn√©s.
  - **Decord** et **PyAV** pour l'extraction rapide des frames.
  
- **Pertes utilis√©es :**
  - **BCE** pour la baseline.
  - **ASL (Asymmetric Loss)** et **Class-Balanced Focal Loss** pour le multi-label long-tailed.
  
- **Optimisation :**
  - **AdamW**, **OneCycleLR**, **AMP**, **EMA**, **gradient accumulation**.

---

## ‚ö†Ô∏è **Consid√©rations importantes**

- **Donn√©es d√©s√©quilibr√©es :** QV-Pipe est un dataset **multi-label et long-tailed**, avec des classes rares qui n√©cessitent une gestion particuli√®re des pertes.
- **Super-images :** La transformation vid√©o en super-image 3√ó3 est **la m√©thode la plus performante** avec un gain net de 10 mAP.
- **Ensemble de mod√®les :** Un **ensemble simple** (moyenne des logits) sur 2-3 mod√®les diff√©rents peut apporter un gain de **+2 √† +3 mAP**.

---

##  **Astuces pour am√©liorer la mAP**

- **Pertes :** Tester **ASL vs CB-Focal** et choisir celle qui maximise la mAP.
- **Sampling :** Toujours utiliser un **sampling uniforme** pour extraire les frames (√©viter les biais).
- **Mod√®les :** Privil√©gier les backbones **pr√©-entra√Æn√©s sur ImageNet** (ConvNeXt, NFNet, EfficientNet).
- **Ensemble :** La combinaison de **5 folds** et de mod√®les diff√©rents (NFNet, EffNet, ConvNeXt) donne des r√©sultats solides.

---

## üîó **Liens vers des ressources GitHub utiles**

- [Decord](https://github.com/dmlc/decord) - loader/lecture vid√©o ultra-rapide (FFmpeg/NV codecs). Parfait pour extraire des frames sans charger toute la vid√©o.  
- [PyAV](https://github.com/PyAV-Org/PyAV) - Traitement vid√©o en Python.  
- [iterative-stratification](https://github.com/trent-b/iterative-stratification) - Stratification pour cross-validation multi-label.  
- [video2frame](https://github.com/jinyu121/video2frame) - Outils simples pour extraire des frames de vid√©os.  
- [timm (PyTorch Image Models)](https://github.com/huggingface/pytorch-image-models) - Collection de mod√®les pr√©-entra√Æn√©s.  
- [TResNet (MIIL)](https://github.com/Alibaba-MIIL/TResNet) - Architecture multi-label performante et l√©g√®re.  
- [ML-Decoder (head multi-label)](https://github.com/Alibaba-MIIL/ML_Decoder) - T√™te multi-label pour mod√®les.
- [ASL (Asymmetric Loss)](https://github.com/Alibaba-MIIL/ASL) - Perte Asymmetric Loss pour multi-label long-tailed.  
- [Class-Balanced Loss](https://github.com/vandit15/Class-balanced-loss-pytorch) - Perte Class-Balanced Focal.  
